{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e556049",
   "metadata": {},
   "source": [
    "# VLM - Vision Language Model Examples\n",
    "\n",
    "This notebook demonstrates practical examples of using Vision Language Models (VLMs) that can understand both images and text.\n",
    "\n",
    "## What is a VLM?\n",
    "- **Purpose**: Understanding images + text together\n",
    "- **Input**: Images + Text prompts\n",
    "- **Output**: Text descriptions/answers\n",
    "- **Examples**: GPT-4V, Claude 3, LLaVA, Gemini Vision\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111480e8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2440c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai anthropic pillow requests matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f57117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "anthropic_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c1f11",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Encode image to base64\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def download_sample_image(url):\n",
    "    \"\"\"Download and display sample image\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img\n",
    "\n",
    "def display_image(img):\n",
    "    \"\"\"Display image with matplotlib\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c3f3f",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 1: Image Description\n",
    "\n",
    "Get detailed descriptions of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_image_gpt4v(image_url, prompt=\"Describe this image in detail.\"):\n",
    "    \"\"\"Describe image using GPT-4 Vision\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example with a sample image\n",
    "sample_url = \"https://images.unsplash.com/photo-1517849845537-4d257902454a?w=800\"\n",
    "\n",
    "print(\"Image:\")\n",
    "img = download_sample_image(sample_url)\n",
    "display_image(img)\n",
    "\n",
    "print(\"\\nVLM Description:\")\n",
    "description = describe_image_gpt4v(sample_url)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da27a69",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 2: Visual Question Answering (VQA)\n",
    "\n",
    "Ask specific questions about images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2403027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_about_image(image_url, question):\n",
    "    \"\"\"Ask questions about an image\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": question},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Multiple questions about the same image\n",
    "questions = [\n",
    "    \"What animals are in this image?\",\n",
    "    \"What is the setting or environment?\",\n",
    "    \"What colors are prominent in this image?\",\n",
    "    \"What mood or atmosphere does this image convey?\"\n",
    "]\n",
    "\n",
    "print(\"Image:\")\n",
    "display_image(img)\n",
    "\n",
    "print(\"\\nQuestions and Answers:\")\n",
    "for q in questions:\n",
    "    answer = ask_about_image(sample_url, q)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc41e0",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 3: OCR and Text Reading\n",
    "\n",
    "Extract text from images (documents, signs, menus, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b94024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image_url):\n",
    "    \"\"\"Extract all text from an image\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Extract all text from this image. Maintain formatting and structure.\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example with a document or sign image\n",
    "# Note: Replace with actual image URL containing text\n",
    "print(\"This example would work with an image containing text (menu, sign, document, etc.)\")\n",
    "print(\"\\nExample workflow:\")\n",
    "print(\"1. Provide image with text\")\n",
    "print(\"2. VLM reads and extracts all text\")\n",
    "print(\"3. Text is returned in structured format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a31d5d",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 4: Image Comparison\n",
    "\n",
    "Compare multiple images and identify differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_images(image_url1, image_url2):\n",
    "    \"\"\"Compare two images\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Compare these two images. What are the similarities and differences?\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url1}},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url2}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=400\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example with two different images\n",
    "url1 = \"https://images.unsplash.com/photo-1517849845537-4d257902454a?w=400\"\n",
    "url2 = \"https://images.unsplash.com/photo-1548199973-03cce0bbc87b?w=400\"\n",
    "\n",
    "print(\"Image 1:\")\n",
    "img1 = download_sample_image(url1)\n",
    "display_image(img1)\n",
    "\n",
    "print(\"\\nImage 2:\")\n",
    "img2 = download_sample_image(url2)\n",
    "display_image(img2)\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "comparison = compare_images(url1, url2)\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8a49a",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 5: Chart and Graph Analysis\n",
    "\n",
    "Analyze data visualizations and extract insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chart(image_url):\n",
    "    \"\"\"Analyze charts and extract data/insights\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"\"\"Analyze this chart or graph:\n",
    "                    1. What type of visualization is it?\n",
    "                    2. What data does it show?\n",
    "                    3. What are the key trends or insights?\n",
    "                    4. What conclusions can be drawn?\"\"\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=400\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Chart analysis example - would work with actual chart/graph images\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"‚úì Identify chart type (bar, line, pie, scatter, etc.)\")\n",
    "print(\"‚úì Extract data points and values\")\n",
    "print(\"‚úì Identify trends and patterns\")\n",
    "print(\"‚úì Provide insights and conclusions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab64d7ee",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 6: Object Detection and Counting\n",
    "\n",
    "Identify and count specific objects in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d36318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_objects(image_url, object_type):\n",
    "    \"\"\"Count specific objects in an image\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": f\"How many {object_type} are in this image? List their locations and characteristics.\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example\n",
    "print(\"Counting dogs in image:\")\n",
    "display_image(img)\n",
    "result = count_objects(sample_url, \"dogs\")\n",
    "print(f\"\\nResult:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395390d3",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 7: Image Accessibility (Alt Text Generation)\n",
    "\n",
    "Generate descriptive alt text for accessibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alt_text(image_url, length=\"medium\"):\n",
    "    \"\"\"Generate accessibility-friendly alt text\"\"\"\n",
    "    length_prompts = {\n",
    "        \"short\": \"Generate a concise alt text (1 sentence) for accessibility.\",\n",
    "        \"medium\": \"Generate a descriptive alt text (2-3 sentences) for accessibility.\",\n",
    "        \"long\": \"Generate a detailed alt text describing all important elements for visually impaired users.\"\n",
    "    }\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": length_prompts[length]},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Generate different lengths of alt text\n",
    "print(\"Image:\")\n",
    "display_image(img)\n",
    "\n",
    "for length in [\"short\", \"medium\", \"long\"]:\n",
    "    print(f\"\\n{length.upper()} Alt Text:\")\n",
    "    print(generate_alt_text(sample_url, length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5993aa1",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 8: Product Analysis for E-commerce\n",
    "\n",
    "Analyze product images for descriptions and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_product(image_url):\n",
    "    \"\"\"Analyze product image for e-commerce\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"\"\"Analyze this product image and provide:\n",
    "                    1. Product type and category\n",
    "                    2. Key features and characteristics\n",
    "                    3. Suggested product title\n",
    "                    4. Marketing description (2-3 sentences)\n",
    "                    5. Estimated price range\"\"\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=400\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Product Analysis Example\")\n",
    "print(\"\\nCapabilities for E-commerce:\")\n",
    "print(\"‚úì Automatic product categorization\")\n",
    "print(\"‚úì Feature extraction\")\n",
    "print(\"‚úì Title and description generation\")\n",
    "print(\"‚úì Quality assessment\")\n",
    "print(\"‚úì Price range estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188d673",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 9: Using Claude 3 Vision (Alternative VLM)\n",
    "\n",
    "Compare with Anthropic's Claude 3 Vision model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8765dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_image_claude(image_url, prompt=\"Describe this image.\"):\n",
    "    \"\"\"Describe image using Claude 3\"\"\"\n",
    "    # Download image and encode\n",
    "    response = requests.get(image_url)\n",
    "    image_data = base64.b64encode(response.content).decode('utf-8')\n",
    "    \n",
    "    # Determine media type from URL\n",
    "    media_type = \"image/jpeg\"\n",
    "    if \".png\" in image_url:\n",
    "        media_type = \"image/png\"\n",
    "    \n",
    "    message = anthropic_client.messages.create(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=300,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": media_type,\n",
    "                            \"data\": image_data,\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "# Compare GPT-4V vs Claude 3\n",
    "print(\"Comparing VLM Models:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "prompt = \"Describe this image in detail.\"\n",
    "\n",
    "try:\n",
    "    print(\"\\nGPT-4 Vision:\")\n",
    "    gpt4_response = describe_image_gpt4v(sample_url, prompt)\n",
    "    print(gpt4_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "try:\n",
    "    print(\"\\nClaude 3:\")\n",
    "    claude_response = describe_image_claude(sample_url, prompt)\n",
    "    print(claude_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d2b0f0",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated VLM capabilities:\n",
    "\n",
    "1. ‚úÖ **Image Description** - Detailed scene understanding\n",
    "2. ‚úÖ **Visual Q&A** - Answer specific questions about images\n",
    "3. ‚úÖ **OCR** - Text extraction from images\n",
    "4. ‚úÖ **Image Comparison** - Identify similarities and differences\n",
    "5. ‚úÖ **Chart Analysis** - Extract insights from visualizations\n",
    "6. ‚úÖ **Object Detection** - Count and locate objects\n",
    "7. ‚úÖ **Accessibility** - Generate alt text for screen readers\n",
    "8. ‚úÖ **Product Analysis** - E-commerce descriptions\n",
    "9. ‚úÖ **Model Comparison** - GPT-4V vs Claude 3\n",
    "\n",
    "### Key Takeaways:\n",
    "- VLMs combine vision and language understanding\n",
    "- Can handle complex visual reasoning tasks\n",
    "- Useful for accessibility, e-commerce, analysis\n",
    "- Different models have different strengths\n",
    "\n",
    "### Use Cases:\n",
    "- üì∏ Content moderation\n",
    "- üõí E-commerce product descriptions\n",
    "- ‚ôø Accessibility (alt text generation)\n",
    "- üìä Data visualization analysis\n",
    "- üìÑ Document understanding (OCR + context)\n",
    "- üîç Visual search and categorization\n",
    "\n",
    "### Limitations:\n",
    "- Image resolution limits\n",
    "- Cannot generate images (only understand them)\n",
    "- May hallucinate details\n",
    "- Processing time longer than text-only LLMs\n",
    "\n",
    "### Next Steps:\n",
    "- Explore LMM for multi-modal tasks (audio + video)\n",
    "- Try SAM for image segmentation\n",
    "- Learn about LAM for taking actions based on visual input"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
