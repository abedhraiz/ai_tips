{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53bf0657",
   "metadata": {},
   "source": [
    "# AI Models & Communication - Notebook Index\n",
    "\n",
    "Welcome to the AI Tips comprehensive notebook collection! This index helps you navigate through all available notebooks with examples for different AI model architectures and communication protocols.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad5b2d",
   "metadata": {},
   "source": [
    "## üìö Available Notebooks\n",
    "\n",
    "### 1. [LLM - Large Language Model Examples](./01_LLM_Examples.ipynb)\n",
    "**Text-only AI models for generation and understanding**\n",
    "\n",
    "**Topics Covered:**\n",
    "- Text generation (creative writing, haikus)\n",
    "- Question answering\n",
    "- Text summarization\n",
    "- Code generation\n",
    "- Sentiment analysis\n",
    "- Language translation\n",
    "- Information extraction\n",
    "- Conversational AI\n",
    "- Model comparison (GPT vs Claude)\n",
    "\n",
    "**Models**: GPT-4, Claude, Llama, Gemini\n",
    "\n",
    "---\n",
    "\n",
    "### 2. [VLM - Vision Language Model Examples](./02_VLM_Examples.ipynb)\n",
    "**Models that understand both images and text**\n",
    "\n",
    "**Topics Covered:**\n",
    "- Image description and captioning\n",
    "- Visual Question Answering (VQA)\n",
    "- OCR and text reading from images\n",
    "- Image comparison\n",
    "- Chart and graph analysis\n",
    "- Object detection and counting\n",
    "- Alt text generation for accessibility\n",
    "- Product analysis for e-commerce\n",
    "- Using multiple VLMs (GPT-4V, Claude 3)\n",
    "\n",
    "**Models**: GPT-4V, Claude 3, LLaVA, Gemini Vision\n",
    "\n",
    "---\n",
    "\n",
    "### 3. [SLM - Small Language Model Examples](./03_SLM_Examples.ipynb)\n",
    "**Efficient, compact models for edge devices and fast inference**\n",
    "\n",
    "**Topics Covered:**\n",
    "- Running Phi-3 (3.8B) locally\n",
    "- TinyLlama (1.1B) for extreme efficiency\n",
    "- Code generation with small models\n",
    "- Performance benchmarking\n",
    "- Text classification\n",
    "- 4-bit quantization for memory savings\n",
    "- Use cases and comparison with LLMs\n",
    "\n",
    "**Models**: Phi-3, Gemma, TinyLlama, Mistral 7B\n",
    "\n",
    "---\n",
    "\n",
    "### 4. [Advanced Models: MOE, SAM, LCM, MLM](./04_Advanced_Models_MOE_SAM_LCM_MLM.ipynb)\n",
    "**Specialized architectures for specific tasks**\n",
    "\n",
    "**Topics Covered:**\n",
    "\n",
    "**MOE (Mixture of Experts)**\n",
    "- Efficient scaling with expert routing\n",
    "- Mixtral 8x7B architecture\n",
    "- Routing simulation\n",
    "\n",
    "**SAM (Segment Anything Model)**\n",
    "- Universal image segmentation\n",
    "- Multiple prompting options\n",
    "- Use cases in various industries\n",
    "\n",
    "**LCM (Latent Consistency Model)**\n",
    "- Ultra-fast image generation\n",
    "- 4-step vs 50-step comparison\n",
    "- Real-time AI art\n",
    "\n",
    "**MLM (Masked Language Model)**\n",
    "- BERT for text classification\n",
    "- Fill-mask tasks\n",
    "- Named Entity Recognition (NER)\n",
    "- Question answering\n",
    "\n",
    "---\n",
    "\n",
    "### 5. [Multimodal Models & AI Communication](./05_Multimodal_And_Communication.ipynb)\n",
    "**Advanced multimodal models and inter-AI communication protocols**\n",
    "\n",
    "**Topics Covered:**\n",
    "\n",
    "**LMM & MLLM**\n",
    "- Image + text multimodal queries\n",
    "- Multiple image analysis\n",
    "- Document understanding\n",
    "\n",
    "**Communication Protocols**\n",
    "- **MCP** (Model Context Protocol): AI ‚Üî Tools\n",
    "- **A2A** (Agent-to-Agent): AI ‚Üî AI collaboration\n",
    "- **A2P** (Agent-to-Person): AI ‚Üî Human interaction\n",
    "- Multi-agent orchestration\n",
    "- Complete workflow examples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e21692",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Quick Reference Guide\n",
    "\n",
    "### By Task Type\n",
    "\n",
    "| Task | Recommended Notebook | Model Type |\n",
    "|------|---------------------|------------|\n",
    "| Text generation | #1 LLM | LLM |\n",
    "| Image understanding | #2 VLM | VLM |\n",
    "| Fast inference | #3 SLM | SLM |\n",
    "| Image segmentation | #4 Advanced | SAM |\n",
    "| Fast image generation | #4 Advanced | LCM |\n",
    "| Text classification | #4 Advanced | MLM |\n",
    "| Multi-modal tasks | #5 Multimodal | LMM/MLLM |\n",
    "| Agent collaboration | #5 Multimodal | A2A |\n",
    "\n",
    "### By Use Case\n",
    "\n",
    "| Use Case | Start With |\n",
    "|----------|------------|\n",
    "| Build a chatbot | #1 LLM Examples |\n",
    "| Analyze images | #2 VLM Examples |\n",
    "| Deploy on mobile | #3 SLM Examples |\n",
    "| Remove backgrounds | #4 SAM (Advanced) |\n",
    "| Generate art quickly | #4 LCM (Advanced) |\n",
    "| Extract entities | #4 MLM (Advanced) |\n",
    "| Multi-agent system | #5 Communication |\n",
    "| Tool integration | #5 MCP (Communication) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b52be4",
   "metadata": {},
   "source": [
    "## üöÄ Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install common dependencies\n",
    "pip install openai anthropic transformers torch diffusers pillow matplotlib\n",
    "```\n",
    "\n",
    "### API Keys Required\n",
    "\n",
    "For notebooks #1, #2, and #5, you'll need:\n",
    "\n",
    "```bash\n",
    "# Create a .env file in the root directory\n",
    "OPENAI_API_KEY=your_openai_key_here\n",
    "ANTHROPIC_API_KEY=your_anthropic_key_here\n",
    "```\n",
    "\n",
    "### Notebooks #3 and #4\n",
    "\n",
    "Can run completely locally without API keys! Perfect for:\n",
    "- Learning without costs\n",
    "- Privacy-sensitive applications\n",
    "- Offline development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48cf13",
   "metadata": {},
   "source": [
    "## üìñ Learning Path\n",
    "\n",
    "### Beginner Track\n",
    "1. Start with **#1 LLM Examples** - Learn text AI basics\n",
    "2. Move to **#2 VLM Examples** - Add vision capabilities\n",
    "3. Try **#3 SLM Examples** - Learn about efficiency\n",
    "\n",
    "### Intermediate Track\n",
    "4. Explore **#4 Advanced Models** - Specialized architectures\n",
    "5. Study **#5 Communication** - Multi-agent systems\n",
    "\n",
    "### Advanced Track\n",
    "- Build multi-agent systems combining multiple model types\n",
    "- Implement custom MCP servers\n",
    "- Create production-ready AI applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5891b4",
   "metadata": {},
   "source": [
    "## üí° Model Comparison Table\n",
    "\n",
    "| Model | Input | Output | Size | Speed | Best For |\n",
    "|-------|-------|--------|------|-------|----------|\n",
    "| **LLM** | Text | Text | Large | Medium | General text tasks |\n",
    "| **VLM** | Image+Text | Text | Large | Medium | Visual Q&A |\n",
    "| **SLM** | Text | Text | Small | Fast | Edge devices |\n",
    "| **LMM** | Multi-modal | Multi-modal | Huge | Slow | Complex tasks |\n",
    "| **MLLM** | Multi-modal | Text | Large | Medium | Visual analysis |\n",
    "| **LAM** | UI+Commands | Actions | Medium | Fast | Automation |\n",
    "| **MOE** | Text | Text | Large* | Fast | Multi-domain |\n",
    "| **SAM** | Image+Prompt | Masks | Medium | Fast | Segmentation |\n",
    "| **LCM** | Text prompt | Image | Medium | Very Fast | Image gen |\n",
    "| **MLM** | Text | Labels | Small | Fast | Classification |\n",
    "\n",
    "*MOE: Large total size, but only uses portion during inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c3e67",
   "metadata": {},
   "source": [
    "## üîó Additional Resources\n",
    "\n",
    "### Documentation\n",
    "- [Model Comparison Guide](../docs/MODEL_COMPARISON.md)\n",
    "- [Individual Model Docs](../docs/models/)\n",
    "- [Communication Protocols](../docs/protocols/)\n",
    "\n",
    "### External Links\n",
    "- [Hugging Face Models](https://huggingface.co/models)\n",
    "- [OpenAI Documentation](https://platform.openai.com/docs)\n",
    "- [Anthropic Claude](https://www.anthropic.com/)\n",
    "- [Model Context Protocol](https://modelcontextprotocol.io)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3081a",
   "metadata": {},
   "source": [
    "## ü§ù Contributing\n",
    "\n",
    "Found an issue or want to add examples?\n",
    "\n",
    "1. Open an issue on GitHub\n",
    "2. Submit a pull request\n",
    "3. Share your use cases\n",
    "\n",
    "---\n",
    "\n",
    "## üìù License\n",
    "\n",
    "MIT License - Free to use for learning and development\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! üéâ**\n",
    "\n",
    "Start with any notebook that interests you, and remember:\n",
    "- Run cells sequentially\n",
    "- Experiment with different inputs\n",
    "- Modify examples for your use case\n",
    "- Check documentation for more details"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
