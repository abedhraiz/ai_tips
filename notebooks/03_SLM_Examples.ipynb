{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c006af7",
   "metadata": {},
   "source": [
    "# SLM - Small Language Model Examples\n",
    "\n",
    "Small Language Models are efficient, compact versions of LLMs optimized for speed and resource constraints.\n",
    "\n",
    "## What is an SLM?\n",
    "- **Size**: 100M - 7B parameters\n",
    "- **Purpose**: Efficient text processing on edge devices\n",
    "- **Examples**: Phi-3, Gemma, TinyLlama, Mistral 7B\n",
    "- **Advantages**: Fast, low memory, can run locally\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13c620",
   "metadata": {},
   "source": [
    "## Example 1: Running Phi-3 Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0681f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load Phi-3 Mini (3.8B parameters)\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úì Model loaded successfully!\")\n",
    "print(f\"Model size: ~3.8B parameters\")\n",
    "print(f\"Memory usage: ~7GB (fp16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_slm(prompt, max_length=200):\n",
    "    \"\"\"Generate text using Small Language Model\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "prompt = \"Explain what a neural network is in simple terms.\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Response:\")\n",
    "print(generate_with_slm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba404b36",
   "metadata": {},
   "source": [
    "## Example 2: Code Generation with SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = \"Write a Python function to calculate the factorial of a number.\"\n",
    "\n",
    "print(f\"Prompt: {code_prompt}\\n\")\n",
    "print(\"Generated Code:\")\n",
    "result = generate_with_slm(code_prompt, max_length=300)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e6a61",
   "metadata": {},
   "source": [
    "## Example 3: Running TinyLlama (Even Smaller - 1.1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load TinyLlama - extremely lightweight\n",
    "print(\"Loading TinyLlama (1.1B parameters)...\")\n",
    "tiny_llama = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"‚úì TinyLlama loaded!\")\n",
    "print(\"Memory usage: ~2GB (fp16)\")\n",
    "print(\"Perfect for: Mobile devices, edge computing, real-time applications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752af8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test TinyLlama\n",
    "prompt = \"What are the benefits of machine learning?\"\n",
    "\n",
    "response = tiny_llama(\n",
    "    prompt,\n",
    "    max_length=150,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"TinyLlama Response:\")\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab8bd0",
   "metadata": {},
   "source": [
    "## Example 4: Performance Comparison - SLM vs LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c0c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(model_fn, prompt, name):\n",
    "    \"\"\"Benchmark model performance\"\"\"\n",
    "    start = time.time()\n",
    "    result = model_fn(prompt)\n",
    "    end = time.time()\n",
    "    \n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"time\": end - start,\n",
    "        \"response_length\": len(result),\n",
    "        \"tokens_per_second\": len(result.split()) / (end - start)\n",
    "    }\n",
    "\n",
    "test_prompt = \"Explain photosynthesis.\"\n",
    "\n",
    "# Benchmark TinyLlama\n",
    "def tiny_generate(p):\n",
    "    return tiny_llama(p, max_length=100)[0]['generated_text']\n",
    "\n",
    "tiny_stats = benchmark_model(tiny_generate, test_prompt, \"TinyLlama-1.1B\")\n",
    "\n",
    "print(\"Performance Comparison:\\n\")\n",
    "print(f\"Model: {tiny_stats['model']}\")\n",
    "print(f\"Response time: {tiny_stats['time']:.2f}s\")\n",
    "print(f\"Tokens/sec: {tiny_stats['tokens_per_second']:.1f}\")\n",
    "print(\"\\nNote: SLMs are 5-10x faster than large LLMs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd12ca",
   "metadata": {},
   "source": [
    "## Example 5: Text Classification with SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112ebd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment_slm(text):\n",
    "    \"\"\"Classify sentiment using SLM\"\"\"\n",
    "    prompt = f\"\"\"Classify the sentiment of this text as positive, negative, or neutral:\n",
    "    \n",
    "Text: {text}\n",
    "\n",
    "Sentiment:\"\"\"\n",
    "    \n",
    "    response = tiny_llama(prompt, max_length=20, temperature=0.1)\n",
    "    return response[0]['generated_text'].split(\"Sentiment:\")[-1].strip()\n",
    "\n",
    "# Test sentiment classification\n",
    "texts = [\n",
    "    \"I love this product! It's amazing!\",\n",
    "    \"Terrible experience. Very disappointed.\",\n",
    "    \"The item arrived on time.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    sentiment = classify_sentiment_slm(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b22181",
   "metadata": {},
   "source": [
    "## Example 6: Quantization for Even Smaller Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 4-bit quantization for extreme efficiency\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# 4-bit quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "print(\"Loading quantized model...\")\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úì Quantized model loaded!\")\n",
    "print(\"Memory savings:\")\n",
    "print(\"- FP16: ~7GB\")\n",
    "print(\"- 4-bit: ~2GB (70% reduction!)\")\n",
    "print(\"- Speed: Minimal performance loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37b030",
   "metadata": {},
   "source": [
    "## Example 7: Use Cases for SLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d676f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Perfect Use Cases for SLMs:\\n\")\n",
    "\n",
    "use_cases = {\n",
    "    \"Mobile Apps\": \"Run AI directly on smartphones without cloud\",\n",
    "    \"Edge Devices\": \"IoT devices, smart cameras, embedded systems\",\n",
    "    \"Privacy-Sensitive\": \"Medical, legal, financial (on-premise)\",\n",
    "    \"Real-time Apps\": \"Chatbots, autocomplete, live translation\",\n",
    "    \"Cost Reduction\": \"Lower API costs, no cloud fees\",\n",
    "    \"Offline Apps\": \"Work without internet connection\",\n",
    "    \"Prototyping\": \"Quick testing and development\",\n",
    "    \"Fine-tuning\": \"Easier to customize for specific tasks\"\n",
    "}\n",
    "\n",
    "for use_case, description in use_cases.items():\n",
    "    print(f\"‚Ä¢ {use_case}: {description}\")\n",
    "\n",
    "print(\"\\n‚ùå Not Suitable For:\")\n",
    "print(\"‚Ä¢ Complex reasoning requiring deep context\")\n",
    "print(\"‚Ä¢ Tasks needing extensive world knowledge\")\n",
    "print(\"‚Ä¢ Multi-step complex problem solving\")\n",
    "print(\"‚Ä¢ When accuracy is critical (medical diagnosis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736efb8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### SLM Examples Covered:\n",
    "1. ‚úÖ Running Phi-3 (3.8B) locally\n",
    "2. ‚úÖ TinyLlama (1.1B) for extreme efficiency\n",
    "3. ‚úÖ Performance benchmarking\n",
    "4. ‚úÖ Text classification tasks\n",
    "5. ‚úÖ 4-bit quantization for memory savings\n",
    "6. ‚úÖ Real-world use cases\n",
    "\n",
    "### Key Advantages:\n",
    "- ‚ö° **Speed**: 5-10x faster than large LLMs\n",
    "- üíæ **Memory**: 2-7GB vs 40-200GB for large models\n",
    "- üí∞ **Cost**: No API fees, lower compute costs\n",
    "- üîí **Privacy**: Run completely offline\n",
    "- üì± **Portability**: Works on consumer hardware\n",
    "\n",
    "### Popular SLMs:\n",
    "- **Phi-3** (Microsoft): 3.8B params, excellent quality\n",
    "- **Gemma** (Google): 2B/7B params, open source\n",
    "- **TinyLlama**: 1.1B params, fastest\n",
    "- **Mistral 7B**: 7B params, strong performance\n",
    "- **StableLM**: Various sizes, customizable\n",
    "\n",
    "### When to Choose SLM:\n",
    "‚úÖ Resource constraints (mobile, edge)\n",
    "‚úÖ Real-time requirements\n",
    "‚úÖ Privacy concerns\n",
    "‚úÖ Cost optimization\n",
    "‚úÖ Offline operation needed\n",
    "\n",
    "### Comparison:\n",
    "| Metric | SLM (Phi-3) | LLM (GPT-4) |\n",
    "|--------|-------------|-------------|\n",
    "| Parameters | 3.8B | ~1.7T |\n",
    "| Memory | ~7GB | Cloud only |\n",
    "| Speed | Fast | Slower |\n",
    "| Cost | Free (local) | $$ API |\n",
    "| Quality | Good | Excellent |\n",
    "| Use Case | Specific tasks | General purpose |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
