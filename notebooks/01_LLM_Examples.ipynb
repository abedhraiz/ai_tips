{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51387adc",
   "metadata": {},
   "source": [
    "# LLM - Large Language Model Examples\n",
    "\n",
    "This notebook demonstrates practical examples of using Large Language Models (LLMs) for various text-based tasks.\n",
    "\n",
    "## What is an LLM?\n",
    "- **Purpose**: Text understanding and generation\n",
    "- **Input**: Text only\n",
    "- **Output**: Text only\n",
    "- **Examples**: GPT-4, Claude, Llama, Gemini\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76c3be",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install required packages and set up API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c52ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai anthropic python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50fa0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "anthropic_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e9615",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 1: Text Generation\n",
    "\n",
    "Generate creative content from a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model=\"gpt-3.5-turbo\", max_tokens=200):\n",
    "    \"\"\"Generate text using OpenAI's LLM\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example: Creative Writing\n",
    "prompt = \"Write a short haiku about artificial intelligence.\"\n",
    "result = generate_text(prompt, max_tokens=100)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nOutput:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f890b",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 2: Question Answering\n",
    "\n",
    "Use LLM to answer questions based on its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    \"\"\"Answer questions using LLM\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides accurate, concise answers.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test with multiple questions\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"What are the three laws of robotics?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer_question(q)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a344cb",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 3: Text Summarization\n",
    "\n",
    "Summarize long documents into concise summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7989bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, length=\"short\"):\n",
    "    \"\"\"Summarize text to specified length\"\"\"\n",
    "    length_instructions = {\n",
    "        \"short\": \"in 2-3 sentences\",\n",
    "        \"medium\": \"in a paragraph\",\n",
    "        \"long\": \"in detail with key points\"\n",
    "    }\n",
    "    \n",
    "    prompt = f\"Summarize the following text {length_instructions[length]}:\\n\\n{text}\"\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example text\n",
    "article = \"\"\"\n",
    "Artificial intelligence has revolutionized many industries over the past decade. \n",
    "Machine learning algorithms can now perform tasks that were once thought to be \n",
    "exclusively human, such as image recognition, natural language processing, and \n",
    "strategic game playing. The development of deep learning, particularly neural \n",
    "networks with multiple layers, has been crucial to these advances. Companies \n",
    "across various sectors are investing heavily in AI research and development, \n",
    "leading to innovations in healthcare, finance, transportation, and entertainment. \n",
    "However, these advances also raise important ethical questions about privacy, \n",
    "bias, job displacement, and the future relationship between humans and machines.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original text length:\", len(article.split()), \"words\\n\")\n",
    "print(\"Short Summary:\")\n",
    "print(summarize_text(article, \"short\"))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Medium Summary:\")\n",
    "print(summarize_text(article, \"medium\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef3e5d",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 4: Code Generation\n",
    "\n",
    "Generate code from natural language descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(description, language=\"python\"):\n",
    "    \"\"\"Generate code from description\"\"\"\n",
    "    prompt = f\"Write {language} code to {description}. Include comments and handle errors.\"\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are an expert {language} programmer.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Generate code examples\n",
    "tasks = [\n",
    "    \"calculate fibonacci numbers recursively\",\n",
    "    \"read a CSV file and calculate the average of a numeric column\",\n",
    "    \"create a simple REST API endpoint using Flask\"\n",
    "]\n",
    "\n",
    "for task in tasks:\n",
    "    print(f\"Task: {task}\")\n",
    "    print(\"\\nGenerated Code:\")\n",
    "    print(generate_code(task))\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d00622",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 5: Sentiment Analysis\n",
    "\n",
    "Analyze the sentiment of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyze sentiment of text\"\"\"\n",
    "    prompt = f\"\"\"Analyze the sentiment of the following text. \n",
    "    Respond with JSON containing: sentiment (positive/negative/neutral), \n",
    "    confidence (0-1), and reasoning.\\n\\nText: {text}\"\"\"\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test with various texts\n",
    "texts = [\n",
    "    \"I absolutely love this product! It's amazing and exceeded all my expectations.\",\n",
    "    \"This is the worst experience I've ever had. Very disappointing.\",\n",
    "    \"The weather today is partly cloudy with a chance of rain.\",\n",
    "    \"While the service was good, the food was just okay. Mixed feelings overall.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"Text: {text}\\n\")\n",
    "    print(f\"Analysis: {analyze_sentiment(text)}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec37994",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 6: Language Translation\n",
    "\n",
    "Translate text between languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeafae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, target_language):\n",
    "    \"\"\"Translate text to target language\"\"\"\n",
    "    prompt = f\"Translate the following text to {target_language}:\\n\\n{text}\"\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example translations\n",
    "text = \"Hello, how are you today? I hope you're having a great day!\"\n",
    "languages = [\"Spanish\", \"French\", \"German\", \"Japanese\"]\n",
    "\n",
    "print(f\"Original (English): {text}\\n\")\n",
    "for lang in languages:\n",
    "    translation = translate_text(text, lang)\n",
    "    print(f\"{lang}: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceceef8b",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 7: Information Extraction\n",
    "\n",
    "Extract structured information from unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae21963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(text, info_type):\n",
    "    \"\"\"Extract specific information from text\"\"\"\n",
    "    prompt = f\"\"\"Extract {info_type} from the following text and return as JSON.\\n\\nText: {text}\"\"\"\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example: Extract entities from text\n",
    "text = \"\"\"\n",
    "Apple Inc. announced today that Tim Cook, the CEO, will visit their headquarters \n",
    "in Cupertino, California on December 15th, 2024. The company plans to unveil \n",
    "the new iPhone 16 and discuss Q4 earnings of $89.5 billion.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"\\nExtracted Entities:\")\n",
    "print(extract_information(text, \"named entities (people, organizations, locations, dates, products, money)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd68a08",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 8: Conversational AI\n",
    "\n",
    "Create a multi-turn conversation with context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationBot:\n",
    "    def __init__(self, system_prompt=\"You are a helpful assistant.\"):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        \"\"\"Send message and get response\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=self.messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        \n",
    "        return assistant_message\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"Get conversation history\"\"\"\n",
    "        return self.messages[1:]  # Exclude system message\n",
    "\n",
    "# Create a bot and have a conversation\n",
    "bot = ConversationBot(\"You are a knowledgeable AI assistant specializing in technology.\")\n",
    "\n",
    "conversation = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Can you give me an example?\",\n",
    "    \"How is it different from traditional programming?\"\n",
    "]\n",
    "\n",
    "for user_msg in conversation:\n",
    "    print(f\"User: {user_msg}\")\n",
    "    response = bot.chat(user_msg)\n",
    "    print(f\"Bot: {response}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872882e2",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 9: Comparing Different LLMs\n",
    "\n",
    "Compare outputs from different LLM providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b926db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_llms(prompt):\n",
    "    \"\"\"Compare responses from different LLMs\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # OpenAI GPT\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150\n",
    "        )\n",
    "        results['GPT-3.5'] = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        results['GPT-3.5'] = f\"Error: {e}\"\n",
    "    \n",
    "    # Claude (if API key available)\n",
    "    try:\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=150,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        results['Claude-3-Haiku'] = response.content[0].text\n",
    "    except Exception as e:\n",
    "        results['Claude-3-Haiku'] = f\"Error: {e}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare models\n",
    "prompt = \"Explain the concept of neural networks in one paragraph.\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "results = compare_llms(prompt)\n",
    "for model, response in results.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f12b4c",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated various LLM capabilities:\n",
    "\n",
    "1. ✅ **Text Generation** - Creative writing\n",
    "2. ✅ **Question Answering** - Information retrieval\n",
    "3. ✅ **Summarization** - Condensing long text\n",
    "4. ✅ **Code Generation** - Writing code from descriptions\n",
    "5. ✅ **Sentiment Analysis** - Understanding emotions\n",
    "6. ✅ **Translation** - Language conversion\n",
    "7. ✅ **Information Extraction** - Structured data from text\n",
    "8. ✅ **Conversations** - Multi-turn dialogues\n",
    "9. ✅ **Model Comparison** - Comparing different LLMs\n",
    "\n",
    "### Key Takeaways:\n",
    "- LLMs are versatile for text-only tasks\n",
    "- Temperature controls creativity (0=deterministic, 1=creative)\n",
    "- Context/conversation history improves responses\n",
    "- Different models have different strengths\n",
    "\n",
    "### Next Steps:\n",
    "- Explore VLMs for image + text tasks\n",
    "- Learn about prompt engineering\n",
    "- Try fine-tuning for specific tasks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
